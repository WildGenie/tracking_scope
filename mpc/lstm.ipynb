{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(\"f1data.mat\")\n",
    "\n",
    "x    = mat['x'][:,0]\n",
    "y    = mat['y'][:,0]\n",
    "tail = mat['tail'][:-1,0]\n",
    "\n",
    "input_data = np.stack((tail,x,y)).T\n",
    "output_data = np.stack((x,y)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 500\n",
    "OUTPUT_SIZE = 50\n",
    "BATCH_SIZE  = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tail To Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TailToPosGenerator(tf.keras.utils.Sequence):\n",
    "   \n",
    "    def __init__(self, input_data,output_data , input_window_size , output_window_size,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.window_size = input_window_size\n",
    "        self.output_size = output_window_size\n",
    "        self.EPOCH_LENGHT = 1000\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.x = np.zeros((self.batch_size,self.window_size,3),dtype=np.float32)\n",
    "        self.y = np.zeros((self.batch_size,self.output_size,2),dtype=np.float32)\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.EPOCH_LENGHT\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # [batch, timesteps, feature]\n",
    "\n",
    "        idx = self.index[index]\n",
    "\n",
    "        for i in range(idx,idx+self.batch_size):\n",
    "            in_start  = i\n",
    "            in_stop   = in_start + self.window_size\n",
    "            out_start = in_stop\n",
    "            out_stop  = out_start + self.output_size\n",
    "\n",
    "            self.x[i-idx,:,:] = self.input_data[in_start:in_stop]\n",
    "            self.y[i-idx,:,:] = self.output_data[out_start:out_stop,:]\n",
    "            \n",
    "        return self.x, self.y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = idx = np.random.randint(low=0,high=self.input_data.shape[0] - (self.batch_size + self.window_size + self.output_size)  , size=self.EPOCH_LENGHT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TailToPosLSTMModel(window_size,output_size):\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(window_size,3),name='inpt'),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False,name='lstm'),\n",
    "    tf.keras.layers.Dense(output_size*2,kernel_initializer=tf.initializers.zeros,name='dense'),\n",
    "    tf.keras.layers.Reshape([output_size, 2],name='reshape')\n",
    "    ],name='full_net')\n",
    "\n",
    "    model.compile(\n",
    "                #loss=EuclideanDistanceLoss,\n",
    "                loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistanceLoss(y_true, y_pred):\n",
    "    cumsum_y_true = tf.math.cumsum(y_true, axis=1)\n",
    "    cumsum_y_pred = tf.math.cumsum(y_pred, axis=1)\n",
    "\n",
    "    return tf.norm(cumsum_y_true-cumsum_y_pred, ord='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                17408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 2)             0         \n",
      "=================================================================\n",
      "Total params: 23,908\n",
      "Trainable params: 23,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0582 - mean_absolute_error: 0.1164\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0539 - mean_absolute_error: 0.1100\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0555 - mean_absolute_error: 0.1114\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0534 - mean_absolute_error: 0.1079\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0502 - mean_absolute_error: 0.1073\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0535 - mean_absolute_error: 0.1093\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0556 - mean_absolute_error: 0.1097\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0497 - mean_absolute_error: 0.1075\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0546 - mean_absolute_error: 0.1101\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0517 - mean_absolute_error: 0.1094\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0496 - mean_absolute_error: 0.1070\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0504 - mean_absolute_error: 0.1090\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0502 - mean_absolute_error: 0.1079\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0499 - mean_absolute_error: 0.1069\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0502 - mean_absolute_error: 0.1055\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0458 - mean_absolute_error: 0.1052\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0505 - mean_absolute_error: 0.1082\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0464 - mean_absolute_error: 0.1042\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0556 - mean_absolute_error: 0.1092\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0511 - mean_absolute_error: 0.1092\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0487 - mean_absolute_error: 0.1053\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0476 - mean_absolute_error: 0.1075\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0465 - mean_absolute_error: 0.1059\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0495 - mean_absolute_error: 0.1080\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0482 - mean_absolute_error: 0.1034\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0504 - mean_absolute_error: 0.1076\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0491 - mean_absolute_error: 0.1058\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0479 - mean_absolute_error: 0.1050\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0512 - mean_absolute_error: 0.1056\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0470 - mean_absolute_error: 0.1030\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0483 - mean_absolute_error: 0.1047\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0445 - mean_absolute_error: 0.1026\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0524 - mean_absolute_error: 0.1066\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0454 - mean_absolute_error: 0.1033\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0448 - mean_absolute_error: 0.1037\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0528 - mean_absolute_error: 0.1074\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0445 - mean_absolute_error: 0.1019\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0475 - mean_absolute_error: 0.1051\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0484 - mean_absolute_error: 0.1054\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0457 - mean_absolute_error: 0.1046\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0493 - mean_absolute_error: 0.1058\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0466 - mean_absolute_error: 0.1036\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0525 - mean_absolute_error: 0.1062\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0520 - mean_absolute_error: 0.1075\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0496 - mean_absolute_error: 0.1066\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0461 - mean_absolute_error: 0.1029\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0486 - mean_absolute_error: 0.1047\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0458 - mean_absolute_error: 0.1034\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0461 - mean_absolute_error: 0.1020\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0474 - mean_absolute_error: 0.1040\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0467 - mean_absolute_error: 0.1035\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0489 - mean_absolute_error: 0.1043\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0468 - mean_absolute_error: 0.1029\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0452 - mean_absolute_error: 0.1033\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0435 - mean_absolute_error: 0.1029\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0470 - mean_absolute_error: 0.1046\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0513 - mean_absolute_error: 0.1062\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0445 - mean_absolute_error: 0.1018\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0503 - mean_absolute_error: 0.1069\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0477 - mean_absolute_error: 0.1033\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0521 - mean_absolute_error: 0.1055\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0453 - mean_absolute_error: 0.1021\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0476 - mean_absolute_error: 0.1045\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0497 - mean_absolute_error: 0.1059\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0490 - mean_absolute_error: 0.1042\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0480 - mean_absolute_error: 0.1064\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0456 - mean_absolute_error: 0.1036\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0499 - mean_absolute_error: 0.1052\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0509 - mean_absolute_error: 0.1050\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0467 - mean_absolute_error: 0.1037\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0464 - mean_absolute_error: 0.1044\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0434 - mean_absolute_error: 0.1021\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0489 - mean_absolute_error: 0.1063\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0429 - mean_absolute_error: 0.1009\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0456 - mean_absolute_error: 0.1022\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0472 - mean_absolute_error: 0.1044\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0474 - mean_absolute_error: 0.1041\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0475 - mean_absolute_error: 0.1057\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0432 - mean_absolute_error: 0.1003\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0488 - mean_absolute_error: 0.1043\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0446 - mean_absolute_error: 0.1013\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0505 - mean_absolute_error: 0.1036\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0449 - mean_absolute_error: 0.1027\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0433 - mean_absolute_error: 0.1008\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0433 - mean_absolute_error: 0.1024\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0473 - mean_absolute_error: 0.1044\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0468 - mean_absolute_error: 0.1037\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0447 - mean_absolute_error: 0.1027\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0438 - mean_absolute_error: 0.1017\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0469 - mean_absolute_error: 0.1040\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0479 - mean_absolute_error: 0.1039\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0422 - mean_absolute_error: 0.1000\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0436 - mean_absolute_error: 0.1028\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0470 - mean_absolute_error: 0.1033\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0466 - mean_absolute_error: 0.1021\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0442 - mean_absolute_error: 0.1031\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0445 - mean_absolute_error: 0.1028\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0483 - mean_absolute_error: 0.1062\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0491 - mean_absolute_error: 0.1056\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0451 - mean_absolute_error: 0.1031\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0388 - mean_absolute_error: 0.0984\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.0450 - mean_absolute_error: 0.1034\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0452 - mean_absolute_error: 0.1027\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0481 - mean_absolute_error: 0.1039\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0441 - mean_absolute_error: 0.1016\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0483 - mean_absolute_error: 0.1046\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0417 - mean_absolute_error: 0.0998\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0459 - mean_absolute_error: 0.1044\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0494 - mean_absolute_error: 0.1046\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0462 - mean_absolute_error: 0.1028\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.0421 - mean_absolute_error: 0.1003\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0489 - mean_absolute_error: 0.1046\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0452 - mean_absolute_error: 0.1017\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0465 - mean_absolute_error: 0.1040\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0437 - mean_absolute_error: 0.1022\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0474 - mean_absolute_error: 0.1054\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0468 - mean_absolute_error: 0.1040\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.0426 - mean_absolute_error: 0.1017\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0440 - mean_absolute_error: 0.1012\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0417 - mean_absolute_error: 0.1011\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0439 - mean_absolute_error: 0.1026\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0450 - mean_absolute_error: 0.1015\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0435 - mean_absolute_error: 0.1010 1s - loss: 0.0434\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0457 - mean_absolute_error: 0.1028\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0417 - mean_absolute_error: 0.0988\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0468 - mean_absolute_error: 0.1048\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0446 - mean_absolute_error: 0.1020\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0447 - mean_absolute_error: 0.1013\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0438 - mean_absolute_error: 0.1007\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0429 - mean_absolute_error: 0.1014\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0497 - mean_absolute_error: 0.1045\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.0497 - mean_absolute_error: 0.1012\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0437 - mean_absolute_error: 0.1015\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0438 - mean_absolute_error: 0.0996\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0439 - mean_absolute_error: 0.1019\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0477 - mean_absolute_error: 0.1023\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0491 - mean_absolute_error: 0.1051\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0458 - mean_absolute_error: 0.1005 0s - loss: 0.0452 - mean_absolute_error: 0.1\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.0463 - mean_absolute_error: 0.1021\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0460 - mean_absolute_error: 0.1014\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0491 - mean_absolute_error: 0.1033\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0470 - mean_absolute_error: 0.1029\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0436 - mean_absolute_error: 0.1010\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0488 - mean_absolute_error: 0.1037\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0473 - mean_absolute_error: 0.1032\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0416 - mean_absolute_error: 0.0984\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0465 - mean_absolute_error: 0.1023\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0503 - mean_absolute_error: 0.1047\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0440 - mean_absolute_error: 0.1009\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0435 - mean_absolute_error: 0.1004\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0452 - mean_absolute_error: 0.1034\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0466 - mean_absolute_error: 0.1038\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0478 - mean_absolute_error: 0.1015\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0477 - mean_absolute_error: 0.1043\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0430 - mean_absolute_error: 0.1012\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0487 - mean_absolute_error: 0.1041\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0431 - mean_absolute_error: 0.1001\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0476 - mean_absolute_error: 0.1041\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0473 - mean_absolute_error: 0.1037\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0510 - mean_absolute_error: 0.1043\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0472 - mean_absolute_error: 0.1033\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0425 - mean_absolute_error: 0.1004\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0518 - mean_absolute_error: 0.1061\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0425 - mean_absolute_error: 0.1006\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0451 - mean_absolute_error: 0.1027\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0433 - mean_absolute_error: 0.1012\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0417 - mean_absolute_error: 0.1005\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0471 - mean_absolute_error: 0.1029\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0438 - mean_absolute_error: 0.1016\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0478 - mean_absolute_error: 0.1025\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0433 - mean_absolute_error: 0.1023\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0446 - mean_absolute_error: 0.1020\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0447 - mean_absolute_error: 0.1016\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0437 - mean_absolute_error: 0.1007\n",
      "Epoch 175/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0454 - mean_absolute_error: 0.1023\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0441 - mean_absolute_error: 0.1013\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0483 - mean_absolute_error: 0.1058\n",
      "Epoch 178/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0473 - mean_absolute_error: 0.1030\n",
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0459 - mean_absolute_error: 0.1023\n",
      "Epoch 180/1000\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0446 - mean_absolute_error: 0.1023\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0449 - mean_absolute_error: 0.1005\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.0492 - mean_absolute_error: 0.1054\n",
      "Epoch 183/1000\n",
      " 429/1000 [===========>..................] - ETA: 19s - loss: 0.0468 - mean_absolute_error: 0.1036"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c26d87e98e90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mTailToPosGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOUTPUT_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TailToPosLSTMModel( window_size = WINDOW_SIZE, output_size = OUTPUT_SIZE)\n",
    "\n",
    "gen =TailToPosGenerator(input_data,output_data,WINDOW_SIZE,OUTPUT_SIZE,BATCH_SIZE)\n",
    "\n",
    "model.fit(gen,epochs= 1000,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((1,WINDOW_SIZE,3),dtype=np.float32)\n",
    "ty = np.zeros((1,OUTPUT_SIZE,3),dtype=np.float32)\n",
    "\n",
    "start  = 134\n",
    "t[0,:,:] = data[start:start+WINDOW_SIZE]\n",
    "ty[0,:,:] = data[start+WINDOW_SIZE:start+WINDOW_SIZE+OUTPUT_SIZE]\n",
    "\n",
    "y  = model(t)\n",
    "\n",
    "print(t.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(range(window_size))\n",
    "xx = np.asarray(range(window_size,window_size + output_size))\n",
    "\n",
    "\n",
    "N = data.shape[1]\n",
    "for i in range(N):\n",
    "    plt.figure(figsize=(30,2))\n",
    "    plt.suptitle(data.columns[i], fontsize=20)\n",
    "    plt.plot(x,t[0,:,i])\n",
    "    plt.plot(xx,y[0,:,i],color='green')\n",
    "    plt.plot(xx,ty[0,:,i],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(range(WINDOW_SIZE))\n",
    "xx = np.asarray(range(WINDOW_SIZE,WINDOW_SIZE + OUTPUT_SIZE))\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(30,2))\n",
    "    plt.plot(x,  t[0,:,i])\n",
    "    plt.plot(xx,ty[0,:,i],color='green')\n",
    "    plt.plot(xx, y[0,:,i],color='red')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevx = np.cumsum(np.squeeze(t[0,:,0]))\n",
    "prevy = np.cumsum(np.squeeze(t[0,:,1]))\n",
    "\n",
    "trux = prevx[-1] + np.cumsum(np.squeeze(ty[0,:,0]))\n",
    "truy = prevy[-1] + np.cumsum(np.squeeze(ty[0,:,1]))\n",
    "\n",
    "predx = prevx[-1] + np.cumsum(np.squeeze())\n",
    "predy = prevy[-1] + np.cumsum(np.squeeze(y[0,:,1]))\n",
    "\n",
    "\n",
    "plt.plot(prevx,prevy,color='blue')\n",
    "plt.plot(trux,truy,color='green')\n",
    "# plt.plot(predx,predy,color='red')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.05200195 -0.02529907]\n",
      "  [ 0.02697754 -0.04050446]\n",
      "  [-0.05187988  0.02540588]\n",
      "  [ 0.05187988 -0.02540588]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.02697754 -0.04050446]\n",
      "  [-0.05187988  0.02540588]\n",
      "  [ 0.05187988 -0.02540588]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[-0.05187988  0.02540588]\n",
      "  [ 0.05187988 -0.02540588]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.04351807 -0.01239777]]]\n",
      "tf.Tensor(\n",
      "[[[ 0.05200195 -0.02529907]\n",
      "  [ 0.07897949 -0.06580353]\n",
      "  [ 0.02709961 -0.04039764]\n",
      "  [ 0.07897949 -0.06580353]\n",
      "  [ 0.07897949 -0.06580353]]\n",
      "\n",
      " [[ 0.02697754 -0.04050446]\n",
      "  [-0.02490234 -0.01509857]\n",
      "  [ 0.02697754 -0.04050446]\n",
      "  [ 0.02697754 -0.04050446]\n",
      "  [ 0.02697754 -0.04050446]]\n",
      "\n",
      " [[-0.05187988  0.02540588]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.04351807 -0.01239777]]], shape=(3, 5, 2), dtype=float32)\n",
      "tf.Tensor(0.6945555, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen =TailToPosGenerator(input_data,output_data,10,5,3)\n",
    "\n",
    "x,y = gen[0]\n",
    "\n",
    "\n",
    "# print('x',x,x.shape)\n",
    "# print('y',y,y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(y)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "l2_norm = tf.norm(y-y*5, ord='euclidean')\n",
    "\n",
    "print(l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
